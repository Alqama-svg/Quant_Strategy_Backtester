{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e0f922-3e6a-4745-ba92-c1f57e838140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantX FINAL Backtest | RUN_MODE=FULL_YEAR\n",
      "Running 32 tickers: ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'META', 'NVDA', 'JPM', 'BAC', 'GS', 'MS', 'WFC', 'JNJ', 'UNH', 'PFE', 'MRK', 'ABT', 'XOM', 'CVX', 'CAT', 'GE', 'MMM', 'PG', 'KO', 'WMT', 'MCD', 'NKE', 'PEP', 'HD', 'DIS', 'V', 'MA', 'ZTS']\n",
      "Date range: 2024-01-02 → 2024-12-31\n",
      "\n",
      "🚀 Running QuantX V4.9 MD Report (SEQUENTIAL)\n",
      "\n",
      "Processing 20240102\n",
      "Processing 20240103\n",
      "Processing 20240104\n",
      "Processing 20240105\n",
      "Processing 20240108\n",
      "Processing 20240109\n",
      "Processing 20240110\n",
      "Processing 20240111\n",
      "Processing 20240112\n",
      "Processing 20240115\n",
      "Processing 20240116\n",
      "Processing 20240117\n",
      "Processing 20240118\n",
      "Processing 20240119\n",
      "Processing 20240122\n",
      "Processing 20240123\n",
      "Processing 20240124\n",
      "Processing 20240125\n",
      "Processing 20240126\n",
      "Processing 20240129\n",
      "Processing 20240130\n",
      "Processing 20240131\n",
      "Processing 20240201\n",
      "Processing 20240202\n",
      "Processing 20240205\n",
      "Processing 20240206\n",
      "Processing 20240207\n",
      "Processing 20240208\n",
      "Processing 20240209\n",
      "Processing 20240212\n",
      "Processing 20240213\n",
      "Processing 20240214\n",
      "Processing 20240215\n",
      "Processing 20240216\n",
      "Processing 20240219\n",
      "Processing 20240220\n",
      "Processing 20240221\n",
      "Processing 20240222\n",
      "Processing 20240223\n",
      "Processing 20240226\n",
      "Processing 20240227\n",
      "Processing 20240228\n",
      "Processing 20240229\n",
      "Processing 20240301\n",
      "Processing 20240304\n",
      "Processing 20240305\n",
      "Processing 20240306\n",
      "Processing 20240307\n",
      "Processing 20240308\n",
      "Processing 20240311\n",
      "Processing 20240312\n",
      "Processing 20240313\n",
      "Processing 20240314\n",
      "Processing 20240315\n",
      "Processing 20240318\n",
      "Processing 20240319\n",
      "Processing 20240320\n",
      "Processing 20240321\n",
      "Processing 20240322\n",
      "Processing 20240325\n",
      "Processing 20240326\n",
      "Processing 20240327\n",
      "Processing 20240328\n",
      "Processing 20240329\n",
      "Processing 20240401\n",
      "Processing 20240402\n",
      "Processing 20240403\n",
      "Processing 20240404\n",
      "Processing 20240405\n",
      "Processing 20240408\n",
      "Processing 20240409\n",
      "Processing 20240410\n",
      "Processing 20240411\n",
      "Processing 20240412\n",
      "Processing 20240415\n",
      "Processing 20240416\n",
      "Processing 20240417\n",
      "Processing 20240418\n",
      "Processing 20240419\n",
      "Processing 20240422\n",
      "Processing 20240423\n",
      "Processing 20240424\n",
      "Processing 20240425\n",
      "Processing 20240426\n",
      "Processing 20240429\n",
      "Processing 20240430\n",
      "Processing 20240501\n",
      "Processing 20240502\n",
      "Processing 20240503\n",
      "Processing 20240506\n",
      "Processing 20240507\n",
      "Processing 20240508\n",
      "Processing 20240509\n",
      "Processing 20240510\n",
      "Processing 20240513\n",
      "Processing 20240514\n",
      "Processing 20240515\n",
      "Processing 20240516\n",
      "Processing 20240517\n",
      "Processing 20240520\n",
      "Processing 20240521\n",
      "Processing 20240522\n",
      "Processing 20240523\n",
      "Processing 20240524\n",
      "Processing 20240527\n",
      "Processing 20240528\n",
      "Processing 20240529\n",
      "Processing 20240530\n",
      "Processing 20240531\n",
      "Processing 20240603\n",
      "Processing 20240604\n",
      "Processing 20240605\n",
      "Processing 20240606\n",
      "Processing 20240607\n",
      "Processing 20240610\n",
      "Processing 20240611\n",
      "Processing 20240612\n",
      "Processing 20240613\n",
      "Processing 20240614\n",
      "Processing 20240617\n",
      "Processing 20240618\n",
      "Processing 20240619\n",
      "Processing 20240620\n",
      "Processing 20240621\n",
      "Processing 20240624\n",
      "Processing 20240625\n",
      "Processing 20240626\n",
      "Processing 20240627\n",
      "Processing 20240628\n",
      "Processing 20240701\n",
      "Processing 20240702\n",
      "Processing 20240703\n",
      "Processing 20240704\n",
      "Processing 20240705\n",
      "Processing 20240708\n",
      "Processing 20240709\n",
      "Processing 20240710\n",
      "Processing 20240711\n",
      "Processing 20240712\n",
      "Processing 20240715\n",
      "Processing 20240716\n",
      "Processing 20240717\n",
      "Processing 20240718\n",
      "Processing 20240719\n",
      "Processing 20240722\n",
      "Processing 20240723\n",
      "Processing 20240724\n",
      "Processing 20240725\n",
      "Processing 20240726\n",
      "Processing 20240729\n",
      "Processing 20240730\n",
      "Processing 20240731\n",
      "Processing 20240801\n",
      "Processing 20240802\n",
      "Processing 20240805\n",
      "Processing 20240806\n",
      "Processing 20240807\n",
      "Processing 20240808\n",
      "Processing 20240809\n",
      "Processing 20240812\n",
      "Processing 20240813\n",
      "Processing 20240814\n",
      "Processing 20240815\n",
      "Processing 20240816\n",
      "Processing 20240819\n",
      "Processing 20240820\n",
      "Processing 20240821\n",
      "Processing 20240822\n",
      "Processing 20240823\n",
      "Processing 20240826\n",
      "Processing 20240827\n",
      "Processing 20240828\n",
      "Processing 20240829\n",
      "Processing 20240830\n",
      "Processing 20240902\n",
      "Processing 20240903\n",
      "Processing 20240904\n",
      "Processing 20240905\n",
      "Processing 20240906\n",
      "Processing 20240909\n",
      "Processing 20240910\n",
      "Processing 20240911\n",
      "Processing 20240912\n",
      "Processing 20240913\n",
      "Processing 20240916\n",
      "Processing 20240917\n",
      "Processing 20240918\n",
      "Processing 20240919\n",
      "Processing 20240920\n",
      "Processing 20240923\n",
      "Processing 20240924\n",
      "Processing 20240925\n",
      "Processing 20240926\n",
      "Processing 20240927\n",
      "Processing 20240930\n",
      "Processing 20241001\n",
      "Processing 20241002\n",
      "Processing 20241003\n",
      "Processing 20241004\n",
      "Processing 20241007\n",
      "Processing 20241008\n",
      "Processing 20241009\n",
      "Processing 20241010\n",
      "Processing 20241011\n",
      "Processing 20241014\n",
      "Processing 20241015\n",
      "Processing 20241016\n",
      "Processing 20241017\n",
      "Processing 20241018\n",
      "Processing 20241021\n",
      "Processing 20241022\n",
      "Processing 20241023\n",
      "Processing 20241024\n",
      "Processing 20241025\n",
      "Processing 20241028\n",
      "Processing 20241029\n",
      "Processing 20241030\n",
      "Processing 20241031\n",
      "Processing 20241101\n",
      "Processing 20241104\n",
      "Processing 20241105\n",
      "Processing 20241106\n",
      "Processing 20241107\n",
      "Processing 20241108\n",
      "Processing 20241111\n",
      "Processing 20241112\n",
      "Processing 20241113\n",
      "Processing 20241114\n",
      "Processing 20241115\n",
      "Processing 20241118\n",
      "Processing 20241119\n",
      "Processing 20241120\n",
      "Processing 20241121\n",
      "Processing 20241122\n",
      "Processing 20241125\n",
      "Processing 20241126\n",
      "Processing 20241127\n",
      "Processing 20241128\n",
      "Processing 20241129\n",
      "Processing 20241202\n",
      "Processing 20241203\n",
      "Processing 20241204\n",
      "Processing 20241205\n",
      "Processing 20241206\n",
      "Processing 20241209\n",
      "Processing 20241210\n",
      "Processing 20241211\n",
      "Processing 20241212\n",
      "Processing 20241213\n",
      "Processing 20241216\n",
      "Processing 20241217\n",
      "Processing 20241218\n",
      "Processing 20241219\n",
      "Processing 20241220\n",
      "Processing 20241223\n",
      "Processing 20241224\n",
      "Processing 20241225\n",
      "Processing 20241226\n",
      "Processing 20241227\n",
      "Processing 20241230\n",
      "Processing 20241231\n",
      "✅ Trade report exported: ./quantx_reports\\QuantX_V4.9_Trades_Report.csv\n",
      "\n",
      "--- Diagnostics Summary ---\n",
      "entries: 3308\n",
      "intraday_exits: 55\n",
      "eod_closes: 3253\n",
      "z_fail: 1234512\n",
      "vol_fail: 27\n",
      "trend_fail: 568989\n",
      "confirm_fail: 0\n",
      "size_fail: 48938\n",
      "cash_fail: 353\n",
      "price_fail: 25059\n",
      "missing_data: 0\n",
      "----------------------------\n",
      "\n",
      "Initial capital: $1,000,000.00\n",
      "Final capital:   $1,401,058.10\n",
      "Total return:    40.11%\n",
      "Annualized:      26.30%\n",
      "Max drawdown:    -1.91%\n",
      "Sharpe (ann):    2.36\n",
      "Trades executed: 3308\n",
      "\n",
      "Win rate:        66.63%\n",
      "Avg Win:         480.21,  Avg Loss: -345.07\n",
      "✅ Equity curve saved: ./quantx_reports\\QuantX_V4.9_EquityCurve.png\n",
      "✅ PnL hist saved: ./quantx_reports\\QuantX_V4.9_PnL_Hist.png\n",
      "✅ Per-ticker charts saved (up to 32) in ./quantx_reports\n",
      "✅ Combined PDF report saved: ./quantx_reports\\QuantX_V4.9_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "import os, math, traceback, glob\n",
    "from collections import defaultdict\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import time\n",
    "\n",
    "# ------------------------\n",
    "# User config\n",
    "# ------------------------\n",
    "DATA_ROOT = r\"C:\\Users\\Alqama\\QuantX_Trading_Project\\Data\\1 Min Data\\1 Min Data\\OHLC\"\n",
    "\n",
    "ALL_TICKERS = [\n",
    "    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"META\",\"NVDA\",\n",
    "    \"JPM\",\"BAC\",\"GS\",\"MS\",\"WFC\",\n",
    "    \"JNJ\",\"UNH\",\"PFE\",\"MRK\",\"ABT\",\n",
    "    \"XOM\",\"CVX\",\"CAT\",\"GE\",\"MMM\",\n",
    "    \"PG\",\"KO\",\"WMT\",\"MCD\",\"NKE\",\"PEP\",\"HD\",\"DIS\",\n",
    "    \"V\",\"MA\",\"ZTS\"\n",
    "]\n",
    "\n",
    "RUN_MODE = \"FULL_YEAR\"   # JAN / FEB / JAN_FEB / JAN_JUN / JUL_SEP / OCT_DEC / FULL_YEAR\n",
    "\n",
    "# ------------------------\n",
    "# Strategy params (unchanged)\n",
    "# ------------------------\n",
    "DAILY_TREND_WINDOW    = 5\n",
    "INTRADAY_LOOKBACK     = 15\n",
    "ATR_WINDOW            = 14\n",
    "\n",
    "Z_THRESHOLD           = 0.65\n",
    "CONFIRM_BARS          = 0\n",
    "VOLUME_MIN_FACTOR     = 0.35\n",
    "\n",
    "RISK_PER_TRADE        = 0.035\n",
    "MAX_POSITION_FRACTION = 0.05\n",
    "MAX_GROSS_EXPOSURE    = 2.2\n",
    "MAX_OPEN_POSITIONS_TOTAL = 14\n",
    "\n",
    "STOP_LOSS_PCT         = 0.022\n",
    "TAKE_PROFIT_PCT       = 0.10\n",
    "\n",
    "TRANSACTION_COST_PCT  = 0.0002\n",
    "SLIPPAGE_PCT          = 0.0005\n",
    "MINUTES_PER_DAY       = 390\n",
    "SKIP_FIRST_MINUTES    = 3\n",
    "SKIP_LAST_MINUTES     = 5\n",
    "\n",
    "INITIAL_CAPITAL       = 1_000_000.0\n",
    "\n",
    "ROLL_STD_FLOOR = 1e-4\n",
    "VOL_FLOOR = 1e-4\n",
    "ATR_FLOOR = 1e-4\n",
    "\n",
    "# Output paths & behaviour\n",
    "OUT_DIR = \"./quantx_reports\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "GENERATE_PDF = True\n",
    "MAX_TICKER_CHARTS = 32            # max per-ticker images\n",
    "MAX_TRADES_PER_TICKER_ZOOM = 0    # 0 = disabled (avoid 322 extra charts)\n",
    "\n",
    "# ------------------------\n",
    "# RUN_MODE -> dates\n",
    "# ------------------------\n",
    "if RUN_MODE == \"JAN\":\n",
    "    START_DATE, END_DATE = \"2024-01-02\", \"2024-01-31\"\n",
    "elif RUN_MODE == \"FEB\":\n",
    "    START_DATE, END_DATE = \"2024-02-01\", \"2024-02-29\"\n",
    "elif RUN_MODE == \"JAN_FEB\":\n",
    "    START_DATE, END_DATE = \"2024-01-02\", \"2024-02-29\"\n",
    "elif RUN_MODE == \"JAN_JUN\":\n",
    "    START_DATE, END_DATE = \"2024-01-02\", \"2024-06-28\"\n",
    "elif RUN_MODE == \"JUL_SEP\":\n",
    "    START_DATE, END_DATE = \"2024-07-01\", \"2024-09-30\"\n",
    "elif RUN_MODE == \"OCT_DEC\":\n",
    "    START_DATE, END_DATE = \"2024-10-01\", \"2024-12-31\"\n",
    "else:\n",
    "    START_DATE, END_DATE = \"2024-01-02\", \"2024-12-31\"\n",
    "\n",
    "TICKERS = ALL_TICKERS\n",
    "\n",
    "print(f\"QuantX FINAL Backtest | RUN_MODE={RUN_MODE}\")\n",
    "print(f\"Running {len(TICKERS)} tickers: {TICKERS}\")\n",
    "print(f\"Date range: {START_DATE} → {END_DATE}\\n\")\n",
    "\n",
    "# ------------------------\n",
    "# IO helper (robust & cached)\n",
    "# ------------------------\n",
    "_day_cache = {}\n",
    "def _find_file_for_day(ticker, date_str):\n",
    "    \"\"\"\n",
    "    Look inside DATA_ROOT/ticker for any file containing date_str in its filename and return full path.\n",
    "    Accepts parquet, csv, pkl. Returns None if nothing found.\n",
    "    \"\"\"\n",
    "    folder = os.path.join(DATA_ROOT, ticker)\n",
    "    if not os.path.isdir(folder):\n",
    "        return None\n",
    "    # search for common extensions\n",
    "    for ext in (\"*.parquet\", \"*.parq\", \"*.csv\", \"*.pkl\"):\n",
    "        for fn in glob.glob(os.path.join(folder, f\"*{date_str}*{ext.replace('*','')}\")):\n",
    "            return fn\n",
    "    # fallback: any file with date_str substring\n",
    "    for fn in os.listdir(folder):\n",
    "        if date_str in fn:\n",
    "            return os.path.join(folder, fn)\n",
    "    return None\n",
    "\n",
    "def load_minute_parquet_for_day(ticker, date_str):\n",
    "    \"\"\"\n",
    "    Return pandas DataFrame with columns: timestamp, open, high, low, close, volume, ms_of_day\n",
    "    or None if not available.\n",
    "    \"\"\"\n",
    "    key = (ticker, date_str)\n",
    "    if key in _day_cache:\n",
    "        return _day_cache[key]\n",
    "\n",
    "    candidate = _find_file_for_day(ticker, date_str)\n",
    "    if candidate is None:\n",
    "        _day_cache[key] = None\n",
    "        return None\n",
    "    try:\n",
    "        if candidate.endswith((\".parquet\",\".parq\")):\n",
    "            df_pl = pl.read_parquet(candidate)\n",
    "            df = df_pl.to_pandas()\n",
    "        elif candidate.endswith(\".csv\"):\n",
    "            df = pd.read_csv(candidate)\n",
    "        elif candidate.endswith(\".pkl\"):\n",
    "            df = pd.read_pickle(candidate)\n",
    "        else:\n",
    "            # try read parquet/csv heuristics\n",
    "            try:\n",
    "                df_pl = pl.read_parquet(candidate)\n",
    "                df = df_pl.to_pandas()\n",
    "            except Exception:\n",
    "                df = pd.read_csv(candidate)\n",
    "    except Exception:\n",
    "        _day_cache[key] = None\n",
    "        return None\n",
    "\n",
    "    # normalize columns and construct timestamp (support a couple of naming conventions)\n",
    "    if 'date' not in df.columns or 'ms_of_day' not in df.columns:\n",
    "        # try to infer\n",
    "        if 'timestamp' in df.columns:\n",
    "            # assume timestamp is epoch ms or ISO string\n",
    "            try:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        # if we don't have required columns, give up\n",
    "    # If 'date' exists and 'ms_of_day' exists -> create timestamp\n",
    "    if 'date' in df.columns and 'ms_of_day' in df.columns:\n",
    "        df['date_dt'] = pd.to_datetime(df['date'].astype(str), format=\"%Y%m%d\", errors='coerce')\n",
    "        df['timestamp'] = df['date_dt'] + pd.to_timedelta(df['ms_of_day'], unit='ms')\n",
    "    # else try parse 'timestamp' column (already)\n",
    "    if 'timestamp' not in df.columns:\n",
    "        _day_cache[key] = None\n",
    "        return None\n",
    "\n",
    "    # restrict to market hours if we can\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.loc[\n",
    "            (df['timestamp'].dt.time >= time(9,30)) &\n",
    "            (df['timestamp'].dt.time <= time(16,0))\n",
    "        ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # standardize numeric columns existence\n",
    "    for c in ['open','high','low','close','volume','ms_of_day']:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        _day_cache[key] = None\n",
    "        return None\n",
    "    _day_cache[key] = df[['timestamp','open','high','low','close','volume','ms_of_day']].copy()\n",
    "    return _day_cache[key]\n",
    "\n",
    "# ------------------------\n",
    "# Feature calculators\n",
    "# ------------------------\n",
    "def compute_daily_trend(ticker, dates):\n",
    "    closes = []\n",
    "    for d in dates:\n",
    "        df = load_minute_parquet_for_day(ticker, d.strftime(\"%Y%m%d\"))\n",
    "        closes.append(np.nan if df is None or df.empty else df['close'].iloc[-1])\n",
    "    s = pd.Series(closes, index=dates)\n",
    "    trend = s.rolling(window=DAILY_TREND_WINDOW, min_periods=DAILY_TREND_WINDOW).mean()\n",
    "    return s, trend\n",
    "\n",
    "def compute_intraday_indicators(df_minute):\n",
    "    if df_minute is None or df_minute.empty:\n",
    "        return None\n",
    "    df = df_minute.copy()\n",
    "    df['ret'] = df['close'].pct_change().fillna(0)\n",
    "    df['roll_mean'] = df['close'].rolling(INTRADAY_LOOKBACK, min_periods=INTRADAY_LOOKBACK).mean()\n",
    "    df['roll_std'] = df['close'].rolling(INTRADAY_LOOKBACK, min_periods=INTRADAY_LOOKBACK).std(ddof=0)\n",
    "    df['roll_std'] = df['roll_std'].replace(0, np.nan).ffill().bfill().fillna(ROLL_STD_FLOOR).abs()\n",
    "    df.loc[df['roll_std'] < ROLL_STD_FLOOR, 'roll_std'] = ROLL_STD_FLOOR\n",
    "    df['z'] = (df['close'] - df['roll_mean']) / df['roll_std']\n",
    "    df['vol15'] = df['volume'].rolling(window=15, min_periods=1).mean()\n",
    "    high, low, close = df['high'], df['low'], df['close']\n",
    "    prev_close = close.shift(1).fillna(close)\n",
    "    tr = pd.concat([\n",
    "        (high - low).abs(),\n",
    "        (high - prev_close).abs(),\n",
    "        (low - prev_close).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(window=ATR_WINDOW, min_periods=1).mean().fillna(ATR_FLOOR)\n",
    "    df['volatility'] = (df['atr'] / df['close']).replace([np.inf, -np.inf], np.nan)\n",
    "    df['volatility'] = df['volatility'].ffill().bfill().fillna(VOL_FLOOR).abs()\n",
    "    df.loc[df['volatility'] < VOL_FLOOR, 'volatility'] = VOL_FLOOR\n",
    "    return df\n",
    "\n",
    "# ------------------------\n",
    "# Backtest core (sequential)\n",
    "# ------------------------\n",
    "def run_backtest(tickers, start_date, end_date, stop_on_negative_cash=False):\n",
    "    dates = pd.bdate_range(start=start_date, end=end_date)\n",
    "    daily_closes, daily_trends = {}, {}\n",
    "    for t in tickers:\n",
    "        s, trend = compute_daily_trend(t, dates)\n",
    "        daily_closes[t], daily_trends[t] = s, trend\n",
    "\n",
    "    cash = float(INITIAL_CAPITAL)\n",
    "    positions = {t: {'shares':0, 'entry_price':0.0, 'entry_time':None, 'z':np.nan} for t in tickers}\n",
    "    portfolio_history = [(pd.to_datetime(start_date), cash)]\n",
    "    trades = []\n",
    "    stats = defaultdict(int)\n",
    "\n",
    "    for current_date in dates:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        print(f\"Processing {date_str}\")\n",
    "        try:\n",
    "            intraday = {t: compute_intraday_indicators(load_minute_parquet_for_day(t, date_str)) for t in tickers}\n",
    "            if all(v is None for v in intraday.values()):\n",
    "                portfolio_history.append((pd.to_datetime(current_date) + pd.Timedelta(hours=16), cash))\n",
    "                continue\n",
    "\n",
    "            entered_today = {t: False for t in tickers}\n",
    "            max_len = max((len(df) for df in intraday.values() if df is not None), default=0)\n",
    "            start_i = SKIP_FIRST_MINUTES\n",
    "            end_i = max_len - SKIP_LAST_MINUTES - 1\n",
    "            if end_i <= start_i:\n",
    "                portfolio_history.append((pd.to_datetime(current_date) + pd.Timedelta(hours=16), cash))\n",
    "                continue\n",
    "\n",
    "            for i in range(start_i, end_i):\n",
    "                # mark-to-market price mapping\n",
    "                current_prices = {}\n",
    "                for t, df in intraday.items():\n",
    "                    if df is None or i+1 >= len(df): continue\n",
    "                    p = df['open'].iloc[i+1]\n",
    "                    if np.isfinite(p) and p > 0: current_prices[t] = p\n",
    "\n",
    "                total_value = cash + sum(pos['shares'] * current_prices.get(sym, pos['entry_price'])\n",
    "                                         for sym,pos in positions.items() if pos['shares'] != 0)\n",
    "                gross_exposure = sum(abs(pos['shares']) * current_prices.get(sym, pos['entry_price'])\n",
    "                                     for sym,pos in positions.items() if pos['shares'] != 0)\n",
    "\n",
    "                open_positions_count = sum(1 for p in positions.values() if p['shares'] != 0)\n",
    "                for t, df in intraday.items():\n",
    "                    if df is None or i+1 >= len(df):\n",
    "                        stats['missing_data'] += 1\n",
    "                        continue\n",
    "\n",
    "                    exec_price = df['open'].iloc[i+1]\n",
    "                    if not (np.isfinite(exec_price) and exec_price > 0):\n",
    "                        stats['price_fail'] += 1\n",
    "                        continue\n",
    "\n",
    "                    pos = positions[t]\n",
    "\n",
    "                    # EXIT first (intraday)\n",
    "                    if pos['shares'] != 0:\n",
    "                        pnl_pct = (exec_price - pos['entry_price']) / (pos['entry_price'] if pos['entry_price']!=0 else 1e-8)\n",
    "                        if (pnl_pct <= -STOP_LOSS_PCT) or (pnl_pct >= TAKE_PROFIT_PCT):\n",
    "                            proceeds = pos['shares'] * exec_price * (1.0 - TRANSACTION_COST_PCT - SLIPPAGE_PCT)\n",
    "                            cash += proceeds\n",
    "                            trades.append({\n",
    "                                'symbol': t,\n",
    "                                'entry_price': pos['entry_price'],\n",
    "                                'exit_price': exec_price,\n",
    "                                'entry_time': pos.get('entry_time'),\n",
    "                                'exit_time': df['timestamp'].iloc[i+1],\n",
    "                                'shares': pos['shares'],\n",
    "                                'pnl': (exec_price - pos['entry_price']) * pos['shares'],\n",
    "                                'pnl_pct': pnl_pct,\n",
    "                                'z_score': pos.get('z', np.nan)\n",
    "                            })\n",
    "                            positions[t] = {'shares':0,'entry_price':0.0,'entry_time':None,'z':np.nan}\n",
    "                            stats['intraday_exits'] += 1\n",
    "                            continue\n",
    "\n",
    "                    # ENTRY (only if no open position and not entered today)\n",
    "                    if pos['shares'] == 0 and not entered_today.get(t, False):\n",
    "                        z = df['z'].iloc[i] if 'z' in df.columns else np.nan\n",
    "                        vol15 = df['vol15'].iloc[i] if 'vol15' in df.columns else 0\n",
    "                        median_vol = df['volume'].median() if len(df)>0 else 0\n",
    "                        vol_ok = (median_vol > 0) and (vol15 >= median_vol * VOLUME_MIN_FACTOR)\n",
    "\n",
    "                        today_close = daily_closes[t].loc[current_date] if current_date in daily_closes[t].index else np.nan\n",
    "                        trend_val = daily_trends[t].loc[current_date] if current_date in daily_trends[t].index else np.nan\n",
    "                        trend_bull = pd.notna(today_close) and pd.notna(trend_val) and (today_close > trend_val)\n",
    "\n",
    "                        confirm_pass = True\n",
    "                        if CONFIRM_BARS > 0:\n",
    "                            confirm_pass = True\n",
    "                            for j in range(CONFIRM_BARS):\n",
    "                                idx = i - j\n",
    "                                if idx < 0 or idx >= len(df) or not np.isfinite(df['z'].iloc[idx]) or df['z'].iloc[idx] > -Z_THRESHOLD:\n",
    "                                    confirm_pass = False\n",
    "                                    break\n",
    "\n",
    "                        if not np.isfinite(z):\n",
    "                            stats['z_fail'] += 1; continue\n",
    "                        if z > -Z_THRESHOLD:\n",
    "                            stats['z_fail'] += 1; continue\n",
    "                        if not vol_ok:\n",
    "                            stats['vol_fail'] += 1; continue\n",
    "                        if not trend_bull:\n",
    "                            stats['trend_fail'] += 1; continue\n",
    "                        if not confirm_pass:\n",
    "                            stats['confirm_fail'] += 1; continue\n",
    "\n",
    "                        remaining_capacity = max(0.0, (total_value * MAX_GROSS_EXPOSURE) - gross_exposure)\n",
    "                        if remaining_capacity <= 0:\n",
    "                            stats['size_fail'] += 1; continue\n",
    "\n",
    "                        if not np.isfinite(exec_price) or exec_price <= 0:\n",
    "                            stats['price_fail'] += 1; continue\n",
    "\n",
    "                        vol = df['volatility'].iloc[i] if 'volatility' in df.columns else VOL_FLOOR\n",
    "                        atr = df['atr'].iloc[i] if 'atr' in df.columns else ATR_FLOOR\n",
    "                        if not np.isfinite(vol) or vol <= 0: vol = VOL_FLOOR\n",
    "                        if not np.isfinite(atr) or atr <= 0: atr = ATR_FLOOR\n",
    "\n",
    "                        dollar_risk_per_share = max(atr, exec_price * vol, ATR_FLOOR)\n",
    "                        risk_budget = total_value * RISK_PER_TRADE\n",
    "\n",
    "                        if dollar_risk_per_share <= 0 or not np.isfinite(dollar_risk_per_share):\n",
    "                            stats['size_fail'] += 1; continue\n",
    "\n",
    "                        approx_shares = int(math.floor(risk_budget / dollar_risk_per_share))\n",
    "                        max_shares_by_fraction = int(math.floor((total_value * MAX_POSITION_FRACTION) / max(exec_price, 1e-6)))\n",
    "                        approx_shares = max(0, min(approx_shares, max_shares_by_fraction))\n",
    "                        if approx_shares <= 0:\n",
    "                            stats['size_fail'] += 1; continue\n",
    "\n",
    "                        allowed_value = min(approx_shares * exec_price, remaining_capacity)\n",
    "                        n_shares = int(allowed_value // exec_price)\n",
    "                        if n_shares <= 0:\n",
    "                            stats['size_fail'] += 1; continue\n",
    "\n",
    "                        cost = n_shares * exec_price * (1.0 + TRANSACTION_COST_PCT + SLIPPAGE_PCT)\n",
    "                        if cost > cash:\n",
    "                            stats['cash_fail'] += 1; continue\n",
    "\n",
    "                        if cost > cash * 0.75:\n",
    "                            stats['size_fail'] += 1; continue\n",
    "\n",
    "                        # EXECUTE buy -- store z-score inside position\n",
    "                        cash -= cost\n",
    "                        positions[t] = {'shares': n_shares, 'entry_price': exec_price, 'entry_time': df['timestamp'].iloc[i+1], 'z': float(z)}\n",
    "                        entered_today[t] = True\n",
    "                        stats['entries'] += 1\n",
    "\n",
    "                # mark-to-market snapshot\n",
    "                total_value = cash + sum(pos['shares'] * current_prices.get(sym, pos['entry_price'])\n",
    "                                         for sym,pos in positions.items() if pos['shares'] != 0)\n",
    "                ts = None\n",
    "                for df in intraday.values():\n",
    "                    if df is not None and i+1 < len(df):\n",
    "                        ts = df['timestamp'].iloc[i+1]; break\n",
    "                if ts is not None:\n",
    "                    portfolio_history.append((ts, total_value))\n",
    "\n",
    "                if cash < INITIAL_CAPITAL * 0.01:\n",
    "                    print(f\"[WARN] Cash very low: {cash:.2f} on {date_str} i={i}. Continuing but check sizing.\")\n",
    "\n",
    "            # End of day: close all positions\n",
    "            for sym, pos in list(positions.items()):\n",
    "                if pos['shares'] != 0:\n",
    "                    df = intraday.get(sym)\n",
    "                    if df is not None and len(df) > 0:\n",
    "                        close_price = df['close'].iloc[-1]\n",
    "                        proceeds = pos['shares'] * close_price * (1.0 - TRANSACTION_COST_PCT - SLIPPAGE_PCT)\n",
    "                        cash += proceeds\n",
    "                        trades.append({\n",
    "                            'symbol': sym,\n",
    "                            'entry_price': pos['entry_price'],\n",
    "                            'exit_price': close_price,\n",
    "                            'entry_time': pos.get('entry_time'),\n",
    "                            'exit_time': pd.to_datetime(current_date) + pd.Timedelta(hours=16),\n",
    "                            'shares': pos['shares'],\n",
    "                            'pnl': (close_price - pos['entry_price']) * pos['shares'],\n",
    "                            'pnl_pct': (close_price - pos['entry_price']) / (pos['entry_price'] if pos['entry_price']!=0 else 1e-8),\n",
    "                            'z_score': pos.get('z', np.nan)\n",
    "                        })\n",
    "                        stats['eod_closes'] += 1\n",
    "                    positions[sym] = {'shares':0, 'entry_price':0.0, 'entry_time':None, 'z':np.nan}\n",
    "\n",
    "            portfolio_history.append((pd.to_datetime(current_date) + pd.Timedelta(hours=16), cash))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception on date\", date_str, e)\n",
    "            traceback.print_exc()\n",
    "            portfolio_history.append((pd.to_datetime(current_date) + pd.Timedelta(hours=16), cash))\n",
    "            continue\n",
    "\n",
    "    idx = pd.DatetimeIndex([t for t,_ in portfolio_history])\n",
    "    vals = [v for _,v in portfolio_history]\n",
    "    series = pd.Series(vals, index=idx).sort_index().resample('1min').last().ffill().fillna(INITIAL_CAPITAL)\n",
    "    trades_df = pd.DataFrame(trades)\n",
    "    # return also daily_closes/daily_trends for reporting\n",
    "    return series, trades_df, stats, daily_closes, daily_trends\n",
    "\n",
    "# ------------------------\n",
    "# Performance helpers and reporting\n",
    "# ------------------------\n",
    "def summarize_performance(series, trades_df):\n",
    "    total_return = series.iloc[-1] / series.iloc[0] - 1\n",
    "    days = max(1, (series.index[-1].date() - series.index[0].date()).days)\n",
    "    annualized = (1 + total_return) ** (252/days) - 1\n",
    "    rolling_max = series.expanding(min_periods=1).max()\n",
    "    dd = (series - rolling_max) / rolling_max\n",
    "    max_dd = dd.min()\n",
    "    rets = series.pct_change().dropna()\n",
    "    sharpe = (rets.mean() / rets.std()) * math.sqrt(252*MINUTES_PER_DAY) if rets.std() > 0 else 0\n",
    "    return {'total_return': total_return, 'annualized_return': annualized, 'max_drawdown': max_dd, 'sharpe': sharpe}\n",
    "\n",
    "# ------------------------\n",
    "# Run (main)\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Running (SEQUENTIAL)\\n\")\n",
    "    # run backtest\n",
    "    series, trades_df, stats, daily_closes, daily_trends = run_backtest(TICKERS, START_DATE, END_DATE)\n",
    "\n",
    "    perf = summarize_performance(series, trades_df)\n",
    "\n",
    "    # Export trades CSV in MD's exact format\n",
    "    if not trades_df.empty:\n",
    "        trades_df['date'] = pd.to_datetime(trades_df['entry_time']).dt.date\n",
    "        def safe_daily_sma(row):\n",
    "            try:\n",
    "                sym = row['symbol']; d = pd.Timestamp(row['date'])\n",
    "                if sym in daily_trends and d in daily_trends[sym].index:\n",
    "                    return daily_trends[sym].loc[d]\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "            return np.nan\n",
    "        trades_df['daily_sma'] = trades_df.apply(safe_daily_sma, axis=1)\n",
    "        trades_df['stop_loss'] = trades_df['entry_price'] * (1 - STOP_LOSS_PCT)\n",
    "        trades_df['target'] = trades_df['entry_price'] * (1 + TAKE_PROFIT_PCT)\n",
    "        # z_score should already be present from run_backtest; if not, fill with NaN\n",
    "        trades_df['z_score'] = trades_df.get('z_score', np.nan).astype(float)\n",
    "\n",
    "        final_trades = trades_df[['entry_time','symbol','entry_price','stop_loss','target','pnl','daily_sma','z_score']].copy()\n",
    "        final_trades.columns = ['TimeOfSignal','Ticker','EntryPrice','StopLoss','TargetPrice','RealizedPnL','DailySMAValue','ZScore']\n",
    "        out_csv = os.path.join(OUT_DIR, \"QuantX_V4.9_Trades_Report.csv\")\n",
    "        final_trades.to_csv(out_csv, index=False)\n",
    "        print(f\"✅ Trade report exported: {out_csv}\")\n",
    "    else:\n",
    "        final_trades = pd.DataFrame(columns=['TimeOfSignal','Ticker','EntryPrice','StopLoss','TargetPrice','RealizedPnL','DailySMAValue','ZScore'])\n",
    "        print(\"ℹ️ No trades executed — empty trade report created.\")\n",
    "\n",
    "    # Diagnostics summary\n",
    "    print(\"\\n--- Diagnostics Summary ---\")\n",
    "    print(f\"entries: {stats.get('entries',0)}\")\n",
    "    print(f\"intraday_exits: {stats.get('intraday_exits',0)}\")\n",
    "    print(f\"eod_closes: {stats.get('eod_closes',0)}\")\n",
    "    print(f\"z_fail: {stats.get('z_fail',0)}\")\n",
    "    print(f\"vol_fail: {stats.get('vol_fail',0)}\")\n",
    "    print(f\"trend_fail: {stats.get('trend_fail',0)}\")\n",
    "    print(f\"confirm_fail: {stats.get('confirm_fail',0)}\")\n",
    "    print(f\"size_fail: {stats.get('size_fail',0)}\")\n",
    "    print(f\"cash_fail: {stats.get('cash_fail',0)}\")\n",
    "    print(f\"price_fail: {stats.get('price_fail',0)}\")\n",
    "    print(f\"missing_data: {stats.get('missing_data',0)}\")\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "    print(f\"Initial capital: ${series.iloc[0]:,.2f}\")\n",
    "    print(f\"Final capital:   ${series.iloc[-1]:,.2f}\")\n",
    "    print(f\"Total return:    {perf['total_return']*100:.2f}%\")\n",
    "    print(f\"Annualized:      {perf['annualized_return']*100:.2f}%\")\n",
    "    print(f\"Max drawdown:    {perf['max_drawdown']*100:.2f}%\")\n",
    "    print(f\"Sharpe (ann):    {perf['sharpe']:.2f}\")\n",
    "    print(f\"Trades executed: {len(trades_df)}\\n\")\n",
    "\n",
    "    if not trades_df.empty:\n",
    "        win_rate = (trades_df['pnl'] > 0).mean()\n",
    "        avg_win = trades_df.loc[trades_df['pnl']>0, 'pnl'].mean()\n",
    "        avg_loss = trades_df.loc[trades_df['pnl']<=0, 'pnl'].mean()\n",
    "        print(f\"Win rate:        {win_rate*100:.2f}%\")\n",
    "        print(f\"Avg Win:         {avg_win:.2f},  Avg Loss: {avg_loss:.2f}\")\n",
    "\n",
    "    # Save equity & PnL hist\n",
    "    try:\n",
    "        eq_png = os.path.join(OUT_DIR, \"QuantX_V4.9_EquityCurve.png\")\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.plot(series.index, series.values, linewidth=1)\n",
    "        plt.title(f\"Equity Curve ({RUN_MODE}): {START_DATE} → {END_DATE}\")\n",
    "        plt.ylabel(\"Portfolio value ($)\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(eq_png, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"✅ Equity curve saved: {eq_png}\")\n",
    "\n",
    "        if not trades_df.empty:\n",
    "            hist_png = os.path.join(OUT_DIR, \"QuantX_V4.9_PnL_Hist.png\")\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.hist(trades_df['pnl'].dropna(), bins=50)\n",
    "            plt.title(\"Trade PnL Distribution\")\n",
    "            plt.xlabel(\"PnL\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(hist_png, bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "            print(f\"✅ PnL hist saved: {hist_png}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save equity/pnl charts:\", e)\n",
    "\n",
    "    # Per-ticker charts (one chart per ticker with all entries/exits marked)\n",
    "    saved_charts = []\n",
    "    try:\n",
    "        traded_tickers = sorted(final_trades['Ticker'].unique()) if not final_trades.empty else []\n",
    "        # If you want *all* tickers regardless of trades, change to TICKERS\n",
    "        tickers_to_plot = list(traded_tickers)[:MAX_TICKER_CHARTS]\n",
    "\n",
    "        for sym in tickers_to_plot:\n",
    "            sym_trades = final_trades[final_trades['Ticker'] == sym]\n",
    "            # build daily series for plotting if available, else skip\n",
    "            if sym not in daily_closes or daily_closes[sym].dropna().empty:\n",
    "                # skip if no daily data\n",
    "                continue\n",
    "            price_series = daily_closes[sym].dropna()\n",
    "            fig, ax = plt.subplots(figsize=(12,5))\n",
    "            ax.plot(price_series.index, price_series.values, '-', linewidth=1, label='Daily Close')\n",
    "            # plot markers for each trade\n",
    "            for _, r in sym_trades.iterrows():\n",
    "                try:\n",
    "                    t_entry = pd.to_datetime(r['TimeOfSignal'])\n",
    "                except Exception:\n",
    "                    t_entry = pd.to_datetime(r['TimeOfSignal'], errors='coerce')\n",
    "                entry_y = r['EntryPrice']\n",
    "                ax.scatter([t_entry], [entry_y], marker='^', color='green', s=50, label='Entry' if _==sym_trades.index[0] else \"\")\n",
    "                # for exit, we don't have explicit exit_time in final_trades columns (we stripped it earlier)\n",
    "                # find original record in trades_df to get exit_time/exit_price\n",
    "                orig = trades_df[(trades_df['symbol']==sym) & (pd.to_datetime(trades_df['entry_time'])==t_entry)]\n",
    "                if not orig.empty:\n",
    "                    exit_time = orig['exit_time'].iloc[0]\n",
    "                    exit_price = orig['exit_price'].iloc[0]\n",
    "                    try:\n",
    "                        exit_time = pd.to_datetime(exit_time)\n",
    "                    except Exception:\n",
    "                        exit_time = pd.to_datetime(exit_time, errors='coerce')\n",
    "                    ax.scatter([exit_time], [exit_price], marker='v', color='red', s=50, label='Exit' if _==sym_trades.index[0] else \"\")\n",
    "            ax.set_title(f\"{sym} | All trades ({len(sym_trades)} trades)\")\n",
    "            ax.legend(loc='upper left')\n",
    "            ax.grid(True)\n",
    "            fname = os.path.join(OUT_DIR, f\"chart_{sym}_all_trades.png\")\n",
    "            fig.savefig(fname, bbox_inches='tight', dpi=150)\n",
    "            plt.close(fig)\n",
    "            saved_charts.append(fname)\n",
    "        print(f\"✅ Per-ticker charts saved (up to {MAX_TICKER_CHARTS}) in {OUT_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save per-ticker charts:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Optional: assemble PDF\n",
    "    try:\n",
    "        if GENERATE_PDF:\n",
    "            pdf_path = os.path.join(OUT_DIR, \"QuantX_V4.9_Report.pdf\")\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # first page: text summary\n",
    "                fig, ax = plt.subplots(figsize=(11,8.5))\n",
    "                ax.axis('off')\n",
    "                txt = f\"QuantX V4.9 Report\\nMode: {RUN_MODE}\\nDate range: {START_DATE} → {END_DATE}\\n\\n\"\n",
    "                txt += f\"Initial capital: ${series.iloc[0]:,.2f}\\nFinal capital: ${series.iloc[-1]:,.2f}\\nTotal return: {perf['total_return']*100:.2f}%\\nAnnualized: {perf['annualized_return']*100:.2f}%\\nMax drawdown: {perf['max_drawdown']*100:.2f}%\\nSharpe (ann): {perf['sharpe']:.2f}\\n\\n\"\n",
    "                txt += f\"Trades executed: {len(trades_df)}\\nEntries: {stats.get('entries',0)}  intraday exits: {stats.get('intraday_exits',0)}  eod closes: {stats.get('eod_closes',0)}\\n\"\n",
    "                ax.text(0.01, 0.99, txt, va='top', ha='left', fontsize=10, family='monospace')\n",
    "                pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "                # equity figure\n",
    "                if os.path.exists(eq_png):\n",
    "                    fig = plt.figure(figsize=(11,6))\n",
    "                    img = plt.imread(eq_png)\n",
    "                    plt.imshow(img); plt.axis('off')\n",
    "                    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "                # per-ticker charts (append)\n",
    "                for p in saved_charts:\n",
    "                    fig = plt.figure(figsize=(11,6))\n",
    "                    img = plt.imread(p)\n",
    "                    plt.imshow(img); plt.axis('off')\n",
    "                    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "                # include small table page (top 20 trades)\n",
    "                if not final_trades.empty:\n",
    "                    fig, ax = plt.subplots(figsize=(11,8.5))\n",
    "                    ax.axis('off')\n",
    "                    sample_table = final_trades.head(40)\n",
    "                    table = ax.table(cellText=sample_table.values,\n",
    "                                     colLabels=sample_table.columns,\n",
    "                                     cellLoc='center',\n",
    "                                     loc='center')\n",
    "                    table.auto_set_font_size(False)\n",
    "                    table.set_fontsize(8)\n",
    "                    table.scale(1, 1.5)\n",
    "                    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "            print(f\"✅ Combined PDF report saved: {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to assemble PDF:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
